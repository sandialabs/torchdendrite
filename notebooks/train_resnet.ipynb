{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a15afe-dba8-4fab-8086-7e622c0fb10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from models import DendResNet\n",
    "from models.modules import DendriticConv2d, DendriticLinear\n",
    "\n",
    "### Parameters ###\n",
    "batch_size = 64\n",
    "resolution = 30\n",
    "dt = 0.01\n",
    "workers = 8\n",
    "num_training_steps = 50000\n",
    "evaluation_steps = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9834d8-8dee-48ef-baa8-5fdc0255cbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda:0\"\n",
    "\n",
    "class CifarDendConv(nn.Module):\n",
    "    def __init__(self, resolution=30, dt=0.001, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = DendriticConv2d(in_channels,6,kernel_size=5,stride=1, resolution=resolution, dt=dt)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = DendriticConv2d(6,16,kernel_size=5,stride=1, resolution=resolution, dt=dt)\n",
    "        self.fc1 = DendriticLinear(16*5*5, 120, resolution=resolution, dt=dt)\n",
    "        self.fc2 = DendriticLinear(120, 84, resolution=resolution, dt=dt)\n",
    "        self.fc3 = DendriticLinear(84, 10, resolution=resolution, dt=dt)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = self.pool(F.sigmoid(self.conv1(x)))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = self.pool(F.sigmoid(self.conv2(x)))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = torch.flatten(x,1)\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = self.fc3(x)\n",
    "        # print(torch.abs(x).mean())\n",
    "        return x\n",
    "\n",
    "class CifarConv(nn.Module):\n",
    "    def __init__(self, resolution=30, dt=0.001, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels,6,kernel_size=5,stride=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(6,16,kernel_size=5,stride=1)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = self.pool(F.sigmoid(self.conv1(x)))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = self.pool(F.sigmoid(self.conv2(x)))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = torch.flatten(x,1)\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = F.sigmoid(self.fc1(x))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = F.sigmoid(self.fc2(x))\n",
    "        # print(torch.abs(x).mean())\n",
    "        x = self.fc3(x)\n",
    "        # print(torch.abs(x).mean())\n",
    "        return x\n",
    "        \n",
    "### Define Model ###\n",
    "model = CifarDendConv(resolution=resolution, dt=dt)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "### Define Datasets ###\n",
    "train = CIFAR10('datasets', train=True, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.4914, 0.4822, 0.4465),(0.247, 0.243, 0.261))\n",
    "                           ]))\n",
    "    \n",
    "test = CIFAR10('datasets', train=False, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.4914, 0.4822, 0.4465),(0.247, 0.243, 0.261))\n",
    "                       ]))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train, \n",
    "                          batch_size=batch_size, \n",
    "                          shuffle=True, \n",
    "                          num_workers=workers, \n",
    "                          pin_memory=True)\n",
    "\n",
    "test_loader = DataLoader(test, \n",
    "                         batch_size=batch_size, \n",
    "                         shuffle=False, \n",
    "                         num_workers=workers, \n",
    "                         pin_memory=True)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=0.005)\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(optimizer=optimizer, \n",
    "                                               num_training_steps=num_training_steps, \n",
    "                                               num_warmup_steps=250)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9ae764-e230-4d8c-a60b-a59ee03f0395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca62f8084f3742b2afc7d98ec374ca4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: 42.548736572265625\n",
      "Training Acc: 0.0625\n",
      "Training Loss: 35.269591000925494\n",
      "Training Acc: 0.10346115537848606\n",
      "Training Loss: 29.67610917690985\n",
      "Training Acc: 0.0997692115768463\n",
      "Training Loss: 26.4467038384449\n",
      "Training Acc: 0.09928428761651131\n",
      "Training Loss: 24.239008956855827\n",
      "Training Acc: 0.10141421078921078\n",
      "Training Loss: 22.399147705303776\n",
      "Training Acc: 0.10231814548361311\n",
      "Training Loss: 20.827378056988092\n",
      "Training Acc: 0.10388907395069953\n",
      "Training Loss: 19.39265432118144\n",
      "Training Acc: 0.10522558537978298\n",
      "Training Loss: 18.008123082318704\n",
      "Training Acc: 0.10611100699650175\n",
      "Training Loss: 16.633152545478172\n",
      "Training Acc: 0.10578631719235895\n",
      "Training Loss: 15.306959760613271\n",
      "Training Acc: 0.10583891443422631\n",
      "Training Loss: 14.160390968974484\n",
      "Training Acc: 0.10579675572519084\n",
      "Training Loss: 13.188381321308654\n",
      "Training Acc: 0.10566269576807731\n",
      "Training Loss: 12.357779202920332\n",
      "Training Acc: 0.10510708243617349\n",
      "Training Loss: 11.64405546914302\n",
      "Training Acc: 0.10490752642102256\n",
      "Training Loss: 11.024221745086523\n",
      "Training Acc: 0.10465542521994135\n",
      "Training Loss: 10.481115139475229\n",
      "Training Acc: 0.10438015496125969\n",
      "Training Loss: 10.001307116371411\n",
      "Training Acc: 0.10434677134791814\n",
      "Training Loss: 9.574932624699937\n",
      "Training Acc: 0.10403590868695846\n",
      "Training Loss: 9.193353768976783\n",
      "Training Acc: 0.10400113134077037\n",
      "Training Loss: 8.849648941376048\n",
      "Training Acc: 0.10368551289742052\n",
      "Training Loss: 8.53826208555955\n",
      "Training Acc: 0.10361716815844602\n",
      "Training Loss: 8.255561628638127\n",
      "Training Acc: 0.10330508089438284\n",
      "Training Loss: 7.99702173049627\n",
      "Training Acc: 0.10335159102764736\n",
      "Training Loss: 7.760070159025658\n",
      "Training Acc: 0.10316770121646392\n",
      "Training Loss: 7.542144862809499\n",
      "Training Acc: 0.10311850103983362\n",
      "Training Loss: 7.3408068087518705\n",
      "Training Acc: 0.10308269881556684\n",
      "Training Loss: 7.154502376805869\n",
      "Training Acc: 0.10285976151681232\n",
      "Training Loss: 6.981393880766471\n",
      "Training Acc: 0.10277326453363805\n",
      "Training Loss: 6.820279355279957\n",
      "Training Acc: 0.10288882567921666\n",
      "Training Loss: 6.669933665292992\n",
      "Training Acc: 0.10289669710705239\n",
      "Training Loss: 6.529240807999704\n",
      "Training Acc: 0.10289801315959231\n",
      "Training Loss: 6.3973372427169535\n",
      "Training Acc: 0.10295392763404575\n",
      "Training Loss: 6.273442616160458\n",
      "Training Acc: 0.10293449278875288\n",
      "Training Loss: 6.156881393173024\n",
      "Training Acc: 0.10291068697800258\n",
      "Training Loss: 6.046940718279091\n",
      "Training Acc: 0.10297751685521654\n",
      "Training Loss: 5.943081950680466\n",
      "Training Acc: 0.10287745806021553\n",
      "Training Loss: 5.844858560184829\n",
      "Training Acc: 0.10292468381796563\n",
      "Training Loss: 5.751733104626313\n",
      "Training Acc: 0.10278687769708451\n",
      "Training Loss: 5.66335401707656\n",
      "Training Acc: 0.10276189621577274\n",
      "Training Loss: 5.579414610862255\n",
      "Training Acc: 0.10280534446555345\n",
      "Training Loss: 5.499559415699668\n",
      "Training Acc: 0.10284057652911911\n",
      "Training Loss: 5.423509682306686\n",
      "Training Acc: 0.10277890200933244\n",
      "Training Loss: 5.351027420338625\n",
      "Training Acc: 0.1027622430471584\n",
      "Training Loss: 5.281782516294496\n",
      "Training Acc: 0.10282872011635306\n",
      "Training Loss: 5.215557568190459\n",
      "Training Acc: 0.10290890809705804\n",
      "Training Loss: 5.152243100252476\n",
      "Training Acc: 0.10288099947830623\n",
      "Training Loss: 5.091639174835579\n",
      "Training Acc: 0.10287156412220237\n",
      "Training Loss: 5.033579750206458\n",
      "Training Acc: 0.10292892258978419\n",
      "Training Loss: 4.977874973863205\n",
      "Training Acc: 0.10301327442657743\n",
      "Training Loss: 4.924465227161405\n",
      "Training Acc: 0.10307800375969922\n",
      "Training Loss: 4.873061655960909\n",
      "Training Acc: 0.10321126774370637\n",
      "Training Loss: 4.823702916760838\n",
      "Training Acc: 0.10328532420583032\n"
     ]
    }
   ],
   "source": [
    "training_logs = {\"completed_steps\": [], \n",
    "                 \"training_loss\": [], \n",
    "                 \"testing_loss\": [],\n",
    "                 \"training_acc\": [], \n",
    "                 \"testing_acc\": []}\n",
    "\n",
    "train = True\n",
    "completed_steps = 0\n",
    "train_loss, test_loss, train_acc, test_acc = [], [], [], []\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "while train:\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(X)\n",
    "        \n",
    "    \n",
    "        # print(pred)\n",
    "        # print(y)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        predictions = torch.argmax(pred, axis=1)\n",
    "        accuracy = (predictions == y).sum() / (len(predictions))\n",
    "        train_acc.append(accuracy.item())\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     print(param.grad)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if completed_steps % evaluation_steps == 0:\n",
    "\n",
    "            # model.eval()\n",
    "\n",
    "            # for X, y in tqdm(test_loader):\n",
    "            #     X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            #     with torch.no_grad():\n",
    "            #         pred = model(X)\n",
    "            #     loss = loss_fn(pred, y)\n",
    "            #     test_loss.append(loss.item())\n",
    "\n",
    "            #     predictions = torch.argmax(pred, axis=1)\n",
    "            #     accuracy = (predictions == y).sum() / len(predictions)\n",
    "            #     test_acc.append(accuracy.item())\n",
    "\n",
    "            ### Save Results ###\n",
    "            avg_train_loss = np.mean(train_loss)\n",
    "            # avg_test_loss = np.mean(test_loss)\n",
    "            avg_train_acc = np.mean(train_acc)\n",
    "            # avg_test_acc = np.mean(test_acc)\n",
    "\n",
    "            print(\"Training Loss:\", avg_train_loss)\n",
    "            # print(\"Testing Loss:\", avg_test_loss)\n",
    "            print(\"Training Acc:\", avg_train_acc)\n",
    "            # print(\"Testing Acc:\", avg_test_acc)\n",
    "            \n",
    "            training_logs[\"completed_steps\"].append(completed_steps)\n",
    "            training_logs[\"training_loss\"].append(avg_train_loss)\n",
    "            # training_logs[\"testing_loss\"].append(avg_test_loss)\n",
    "            training_logs[\"training_acc\"].append(avg_train_acc)\n",
    "            # training_logs[\"testing_acc\"].append(avg_test_acc)\n",
    "\n",
    "        completed_steps += 1 \n",
    "        progress_bar.update(1)\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
